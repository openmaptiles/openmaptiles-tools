#!/usr/bin/env python3
"""
Usage:
  download-osm list <service> [-l] [-c]
  download-osm planet [-l] [-p] [-n] [-v] [-j <file> [-k <kv>]...] [-d <file> [-i <z>] [-a <z>] [-x <v>]] [--] [<args>...]
  download-osm url       <url>  [-n] [-v] [-j <file> [-k <kv>]...] [-d <file> [-i <z>] [-a <z>] [-x <v>] [--id <id>]] [--] [<args>...]
  download-osm (geofabrik|osmfr|bbbike) <id> [-l] [-c] [-n] [-v] [-s <file>] [-j <file> [-k <kv>]...] [-d <file> [-i <z>] [-a <z>] [-x <v>]] [--] [<args>...]
  download-osm make-dc <pbf-file> -d <file> [-i <z>] [-a <z>] [-x <v>] [--id <id>]
  download-osm --help
  download-osm --version

Download types:
  planet           Loads latest planet file (50+ GB) from all known mirrors
  url <url>        Loads file from the specific <url>.
                   If <url>.md5 is available, download-osm will use it to validate.
  list <service>   Show available areas for a service. For now only 'geofabrik'.
  geofabrik <id>   Loads file from Geofabrik by ID, where the ID is either
                   "australia-oceania/new-zealand" or just "new-zealand".
                   See https://download.geofabrik.de/
  bbbike <id>      Loads file from BBBike by extract ID, for example "Austin".
                   See https://download.bbbike.org/osm/
  osmfr <id>       Loads file from openstreetmap.fr by extract ID, for example
                   "central-america/costa_rica".  Download will add '-latest.osm.pbf'.
                   See https://download.openstreetmap.fr/extracts
  make-dc          Analyze area data pbf file and generate Docker-Compose file with
                   metadata, e.g. bounding box.  This action can be combined with
                   the area downloads by setting the -d <file> parameter.

Options:
  -p --include-primary  If set, will download from the main osm.org (please avoid
                        using this parameter to reduce the load on the primary server)
  -l --force-latest     Always download the very latest available planet file,
                        even if there are too few mirrors that have it.
                        For Geofabrik, forces catalog re-download.
  -c --no-cache         Do not cache downloaded list of extracts
  -s --state <file>     Download state file and save it to the <file>
  -j --imposm-cfg <f>   Create an imposm configuration json file, and write
                        replication_url param into it.  Use -k any additional values.
                        (requires planet, geofabrik, or osmfr download type)
  -k --kv <key=value>   Add extra key=value params to the imposm config (requires -j)
  -d --make-dc <file>   Create a docker-compose file with the area metadata.
  -x --dc-ver <ver>     Set docker-compose file version to this value.
                        By default will be 2 unless MAKE_DC_VERSION env var is set.
  -i --minzoom=<zoom>   Set minimum zoom when making docker compose file.
                        By default will be 0 unless MIN_ZOOM env var is set.
  -a --maxzoom=<zoom>   Set maximum zoom when making docker compose file.
                        By default will be 7 unless MAX_ZOOM env var is set.
  --id=<area_id>        Name of the area. If not set, will use filename.
                        By default will be pbf file unless OSM_AREA_NAME env var is set.
  -n --dry-run          If set, do all the steps except the actual data download.
                        State file will still be downloaded if --state is set.
  -v --verbose          Print additional debugging information
  --help                Show this screen.
  --version             Show version.

Any parameters after empty ' -- ' param will be passed to aria2c. For example,
this sets the number of maximum concurrent downloads:
    download-osm planet -- --max-concurrent-downloads=2
See https://aria2.github.io/manual/en/html/aria2c.html#options for more options.
By default, aria2c is executed with --checksum (md5 hash) and --split parameters.
Split is used to download with as many streams, as download-osm finds.
Use  --split  or  -s  parameter to override that number. Use --dry-run
to see all parameters downloader will use with aria2c without downloading.

Internal callback mode:  If DOWNLOAD_OSM_DC_FILE env var is set, expects 3 parameters,
where the 3rd parameter is the name of the PDF file. Will run make-dc command.
"""

import asyncio
import json
import os
import re
import subprocess
import sys
from asyncio import sleep
from collections import defaultdict
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Tuple

import aiohttp
import yaml
from aiohttp import ClientSession
from bs4 import BeautifulSoup
# noinspection PyProtectedMember
from docopt import docopt, DocoptExit, __all__ as docopt_funcs
from tabulate import tabulate

import openmaptiles
from openmaptiles.utils import print_err

USER_AGENT = f'OpenMapTiles download-osm {openmaptiles.__version__} ' \
             '(https://github.com/openmaptiles/openmaptiles-tools)'


class Catalog:
    def __init__(self):
        self.sites = [
            Site('GB', 'https://planet.openstreetmap.org/pbf/', avoid_by_default=True),
            Site('DE', 'https://download.bbbike.org/osm/planet/'),
            Site('DE', 'https://ftp.spline.de/pub/openstreetmap/pbf/'),
            Site('DE', 'https://ftp5.gwdg.de'
                       '/pub/misc/openstreetmap/planet.openstreetmap.org/pbf/'),
            Site('JP', 'https://planet.passportcontrol.net/pbf/'),
            Site('NL', 'https://ftp.nluug.nl/maps/planet.openstreetmap.org/pbf/'),
            Site('NL', 'https://ftp.snt.utwente.nl/pub/misc/openstreetmap/'),
            Site('TW', 'https://free.nchc.org.tw/osm.planet/pbf/'),
            Site('US', 'https://ftp.osuosl.org/pub/openstreetmap/pbf/'),
            Site('US', 'https://ftpmirror.your.org/pub/openstreetmap/pbf/'),
        ]

    async def init(self,
                   session: ClientSession, verbose: bool,
                   use_primary: bool, force_latest: bool) -> Tuple[List[str], str]:
        """Load the list of all available sources from all mirror sites,
        and pick the most recent file that is widely available.
        Returns a list of urls and the expected md5 hash.
        """
        print("Retrieving available files...")
        await sleep(0.01)  # Make sure the above print statement prints first
        sources_by_hash: Dict[str, List[Source]] = defaultdict(list)
        await asyncio.wait([v.init(session, verbose) for v in self.sites])
        for site in self.sites:
            for s in site.sources:
                sources_by_hash[s.hash].append(s)
        # Remove "latest" from sources if they have no md5 hash
        # noinspection PyTypeChecker
        no_hash_sources = sources_by_hash.pop(None, [])

        ts_to_hash = self.get_attr_to_hash(sources_by_hash, 'timestamp', 'file date')
        if not ts_to_hash:
            raise ValueError(f"Unable to consistently load data - dates don't match")

        # Sources without md5 can only be used if there is one unique length per md5.
        # If more than one hash has the same length (e.g. download sizes didn't change),
        # we don't know which is which, so we have to ignore them.
        len_to_hash = self.get_attr_to_hash(sources_by_hash, 'file_len', 'length')
        if len_to_hash or not no_hash_sources:
            for src in no_hash_sources:
                if src.file_len in len_to_hash:
                    src.hash = len_to_hash[src.file_len]
                    sources_by_hash[src.hash].append(src)
                else:
                    print(f"WARN: Source {src} has unrecognized file length=" +
                          ("unknown" if src.file_len is None else f"{src.file_len:,}"))
        else:
            print(f"Unable to use sources - unable to match 'latest' without date/md5:")
            for s in no_hash_sources:
                print(s)

        # Pick the best hash to download - should have the largest timestamp,
        # but if the count is too low, use the second most common timestamp.
        for hsh in sources_by_hash:
            # Some sources just have "latest", so for each hash try to get a real date
            # by sorting real dates before the None timestamp,
            # and with the non-None file sizes first.
            sources_by_hash[hsh].sort(
                key=lambda v: (v.timestamp or datetime.max, v.file_len or (1 << 63)))
        # Treat "latest" (None) timestamp as largest value (otherwise sort breaks)
        stats = [(v[0].timestamp or datetime.max, len(v), v[0])
                 for v in sources_by_hash.values()]
        stats.sort(reverse=True)

        print("\nLatest available files:\n")
        info = [dict(date=f"{s[0]:%Y-%m-%d}" if s[0] < datetime.max else "Unknown",
                     number_of_sites=s[1], md5=s[2].hash,
                     size=s[2].size_str())
                for s in stats]
        print(tabulate(info, headers="keys") + '\n')

        if not force_latest and len(stats) > 1 and stats[0][1] * 1.5 < stats[1][1]:
            hash_to_download = stats[1][2].hash
            info = f" because the latest {stats[0][0]:%Y-%m-%d} is not widespread yet"
        else:
            hash_to_download = stats[0][2].hash
            info = ""

        src_list = sources_by_hash[hash_to_download]
        ts = next((v.timestamp for v in src_list if v.timestamp), None)
        ts = f"{ts:%Y-%m-%d}" if ts else "latest (unknown date)"

        if len(src_list) > 2 and not use_primary:
            src_list = [v for v in src_list if not v.site.avoid_by_default]
            info = ' (will not use primary)' + info

        print(f"Will download planet published on {ts}, "
              f"size={src_list[0].size_str()}, md5={src_list[0].hash}, "
              f"using {len(src_list)} sources{info}")
        if verbose:
            print(tabulate([dict(country=s.site.country, url=s.url) for s in src_list],
                           headers="keys") + '\n')

        return [s.url for s in src_list], hash_to_download

    @staticmethod
    def get_attr_to_hash(sources_by_hash, attr_name, attr_desc):
        """Verify that a specific attribute is unique per hash in all sources"""
        attr_to_hash = {}
        for sources in sources_by_hash.values():
            for source in sources:
                attr = getattr(source, attr_name)
                if attr is None:
                    continue
                if attr not in attr_to_hash:
                    attr_to_hash[attr] = source.hash
                elif attr_to_hash[attr] != source.hash:
                    print(f"ERROR: Multiple files with the same {attr_desc}"
                          f" have different hashes:")
                    print(f"* {source}, {attr_desc}={attr}, hash={source.hash}")
                    src = sources_by_hash[attr_to_hash[attr]][0]
                    print(f"* {src}, {attr_desc}={getattr(src, attr_name)}, "
                          f"hash={src.hash}")
                    return None
        return attr_to_hash


class Site:
    re_name = re.compile(r'^planet-(\d{6}|latest)\.osm\.pbf(\.md5)?$')

    def __init__(self, country, url, avoid_by_default=False):
        self.country: str = country
        self.url: str = url
        self.avoid_by_default: bool = avoid_by_default
        self.sources: List[Source] = []

    async def init(self, session: ClientSession, verbose: bool):
        """initialize the self.sources with the relevant Source objects
        by parsing the mirror's site HTML page, and getting all <a> tags"""
        try:
            soup = BeautifulSoup(await fetch(session, self.url), 'html.parser')
            sources = self.parse_hrefs(
                [(v.text.strip(), v['href'].strip())
                 for v in soup.find_all('a') if 'href' in v.attrs],
                verbose)
            await asyncio.wait([v.load_hash(session, verbose) for v in sources] +
                               [v.load_metadata(session, verbose) for v in sources])
            if not sources:
                raise ValueError(f"No sources found")
            if len(sources) > 1 and sources[0].hash == sources[1].hash:
                del sources[0]  # latest is the same as the last one
            self.sources = sources
        except Exception as ex:
            print_err(f"Unable to use {self.country} source {self.url}: {ex}")

    def parse_hrefs(self, items: List[tuple], verbose) -> List['Source']:
        """Convert a list of (name, href) tuples to a list of valid sources,
        including only the two most recent ones, plus the 'latest' if available."""
        all_sources: Dict[str, Source] = {}
        for name, href in sorted(items):
            m = self.re_name.match(name)
            if not m:
                if verbose:
                    print(f"Ignoring unexpected name '{name}' from {self.url}")
                continue
            try:
                url = href if '/' in href else (self.url + href)
                date = m.group(1)
                is_md5 = bool(m.group(2))
                dt = None if date == 'latest' else datetime.strptime(date, '%y%m%d')
                if not is_md5:
                    if date in all_sources:
                        raise ValueError(f"{date} already already exists")
                    all_sources[date] = Source(name, url, dt, self)
                else:
                    if date not in all_sources:
                        raise ValueError(f"md5 file exists, but data file does not")
                    all_sources[date].url_hash = url
            except Exception as ex:
                print_err(f'WARN: {ex}, while parsing {name} from {self.url}')

        # get the last 2 sources that have dates in the name, as well as the "latest"
        latest = all_sources.pop('latest', None)
        result = [all_sources[k]
                  for k in list(sorted(all_sources.keys(), reverse=True))[:2]]
        if latest:
            result.insert(0, latest)
        return result


@dataclass
class Source:
    name: str
    url: str
    timestamp: datetime = None
    site: Site = None
    url_hash: str = None
    hash: str = None
    file_len: int = None

    def __str__(self):
        res = f"{self.name} from {self.url}"
        if self.site:
            res = f"{res} ({self.site.country})"
        return res

    async def load_hash(self, session: ClientSession, verbose: bool):
        if not self.url_hash:
            return
        try:
            if verbose:
                print(f"Getting md5 checksum from {self.url_hash}")
            hash = (await fetch(session, self.url_hash)).strip().split(' ')[0]
            if not re.match(r'^[a-fA-F0-9]{32}$', hash):
                raise ValueError(f"Invalid md5 hash '{hash}'")
            self.hash = hash
        except Exception as ex:
            print_err(f"Unable to load md5 hash for {self}: {ex}")

    async def load_metadata(self, session: ClientSession, verbose: bool):
        if not self.url:
            return
        try:
            if verbose:
                print(f"Getting content length for {self.url}")
            async with session.head(self.url) as resp:
                if resp.status >= 400:
                    raise ValueError(f"Status={resp.status} for HEAD request")
                if 'Content-Length' in resp.headers:
                    self.file_len = int(resp.headers['Content-Length'])
        except Exception as ex:
            print_err(f"Unable to load metadata for {self}: {ex}")

    def size_str(self):
        if self.file_len is None:
            return "Unknown"
        return f"{self.file_len / 1024.0 / 1024:,.1f} MB ({self.file_len:,})"


async def fetch(session: ClientSession, url: str):
    async with session.get(url) as resp:
        if resp.status >= 400:
            raise ValueError(f"Received status={resp.status}")
        return await resp.text()


async def get_geofabrik_list(session: ClientSession, force_latest, no_cache) -> dict:
    list_path = Path(__file__).parent / 'cache' / 'geofabrik.json'
    list_path.parent.mkdir(exist_ok=True)
    data = None
    if not force_latest:
        try:
            data = list_path.read_text(encoding="utf-8")
        except FileNotFoundError:
            pass
    if data is None:
        print(f"Downloading the list of available Geofabrik areas...")
        data = await fetch(session,
                           "https://download.geofabrik.de/index-v1-nogeom.json")
        if not no_cache:
            list_path.write_text(data, encoding='utf-8')
    entries = {v["properties"]["id"]: v["properties"]
               for v in json.loads(data)["features"]}
    warnings = []
    # resolve parents until no more changes are made
    for entry in entries.values():
        full_id = entry["id"]
        try:
            full_name = entry["name"]
            current = entry
            for idx in range(10):  # avoids infinite recursion
                if "parent" in current:
                    current = entries[current["parent"]]
                    full_id = f"{current['id']}/{full_id}"
                    full_name = f"{current['name']} / {full_name}"
                else:
                    break
            else:
                raise ValueError(f"Geofabrik data '{entry['id']}' has infinite parent loop")
            entry["full_id"] = full_id
            entry["full_name"] = full_name
            entry["url"] = entry["urls"]["pbf"]
            entry["url_updates"] = entry["urls"]["updates"]
        except KeyError as key:
            warnings.append(
                f"Unable to parse entry {full_id} - key {key} does not exist")
    if warnings:
        print(f"ERRORS parsing Geofabrik catalog file:", file=sys.stderr)
        for warn in warnings:
            print(f" * {warn}", file=sys.stderr)

    return {v["full_id"]: v
            for v in sorted(entries.values(), key=lambda v: v["full_id"])}


async def save_state_file(session, state_url, state_file):
    state_file = Path(state_file).resolve()
    print(f"Downloading state file {state_url} to {state_file}")
    data = await fetch(session, state_url)
    state_file.parent.mkdir(exist_ok=True)
    state_file.write_text(data, encoding="utf-8")


def save_cfg_file(dry_run, cfg_file, repl_url, extras):
    cfg_file = Path(cfg_file).resolve()
    print(f"{'Would create' if dry_run else 'Creating'} Imposm config file {cfg_file}:")
    if repl_url.endswith("/minute/"):
        interval = "1m"
    elif repl_url.endswith("/hour/"):
        interval = "1h"
    else:
        interval = "24h"
    data = dict(replication_url=repl_url, replication_interval=interval)
    for kv in (extras or []):
        kv = kv.split('=', 1)
        if len(kv) != 2:
            raise SystemExit(
                f"--kv '{kv[0]}' must be in a 'key=value' format")
        data[kv[0]] = kv[1]
    print(tabulate(data.items()))
    if not dry_run:
        cfg_file.write_text(json.dumps(data, indent=2, sort_keys=True) + "\n",
                            encoding="utf-8")


async def prepare_area_download(args, session):
    area_id = (args['<id>'] or args['--id'] or "").strip()
    area_id = area_id if area_id else None
    repl_url = None
    force_latest = args["--force-latest"]
    no_cache = args['--no-cache']
    if (force_latest or no_cache) and not args.geofabrik:
        raise SystemExit(
            f"--force-latest and --no-cache are only supported with geofabrik service")
    if args.url:
        url = args['<url>']
    elif args.bbbike:
        url = f"https://download.bbbike.org/osm/bbbike/{area_id}/{area_id}.osm.pbf"
    elif args.osmfr:
        url = f"http://download.openstreetmap.fr/" \
              f"extracts/{area_id}-latest.osm.pbf"
        if args.state:
            state_url = url.replace("-latest.osm.pbf", ".state.txt")
            await save_state_file(session, state_url, args.state)
        repl_url = f"http://download.openstreetmap.fr/" \
              f"replication/{area_id}/minute/"
    elif args.geofabrik:
        catalog = await get_geofabrik_list(session, force_latest, no_cache)
        if area_id in catalog:
            url = catalog[area_id]["url"]
            repl_url = catalog[area_id]["url_updates"]
        else:
            # If there is no exact match, find anything that ends with key
            reg = re.compile(".*" + re.escape("/" + area_id.lstrip("/")) + "$",
                             re.IGNORECASE)

            def matches(key):
                return bool(reg.match(key))

            area_urls = [(k, v["url"], v["url_updates"])
                         for k, v in catalog.items() if matches(k)]
            if not area_urls:
                raise SystemExit(
                    f"Error: ID '{area_id}' was not found in Geofabrik.\n"
                    "Use 'list geofabrik' to see available extract, "
                    "or try --force-latest to refresh the list of extracts.")
            elif len(area_urls) > 1:
                variants = "\n".join((f"  * {v[0]}" for v in area_urls))
                raise SystemExit(
                    f"More than one ID '{area_id}' was found in "
                    f"Geofabrik, use longer ID:\n{variants}")
            url = area_urls[0][1]
            repl_url = area_urls[0][2]
            if args.state:
                state_url = url.replace("-latest.osm.pbf", "-updates/state.txt")
                await save_state_file(session, state_url, args.state)
    else:
        raise DocoptExit()
    src = Source(area_id or 'raw url', url, url_hash=url + '.md5')
    await asyncio.wait([src.load_hash(session, args.verbose),
                        src.load_metadata(session, args.verbose)])
    print(f"Downloading {src.url} (size={src.size_str()}, md5={src.hash})", flush=True)
    return [src.url], src.hash, area_id, repl_url


def make_docker_compose_file(pbf_file: Path, dc_file: Path,
                             min_zoom=None, max_zoom=None, area=None, ver=None):
    area, min_zoom, max_zoom, dc_ver = normalize_make_dc(area, min_zoom, max_zoom, ver)
    print(f"Extracting metadata from {pbf_file} using osmconvert", flush=True)
    params = ["osmconvert", "--out-statistics", str(pbf_file)]
    res = subprocess.run(params, capture_output=True)
    exit_code = res.returncode
    if exit_code == 0:
        res_text = res.stdout.decode('utf-8')
        # Convert output to a dictionary with non-empty values
        res = {v[0]: v[1] for v in
               [[vv.strip() for vv in v.split(':', 1)] for v in res_text.split('\n')]
               if len(v) == 2 and all(v)}
        lon_min = res["lon min"]
        lon_max = res["lon max"]
        lat_min = res["lat min"]
        lat_max = res["lat max"]
        timestamp_max = res["timestamp max"]
        dc_data = {
            "version": str(dc_ver),
            "services": {
                "generate-vectortiles": {
                    "environment": {
                        "BBOX": f"{lon_min},{lat_min},{lon_max},{lat_max}",
                        "OSM_MAX_TIMESTAMP": timestamp_max,
                        "OSM_AREA_NAME": area or pbf_file.name.split('.', 1)[0],
                        "MIN_ZOOM": min_zoom,
                        "MAX_ZOOM": max_zoom,
                    }}}}
        print(f"Saving metadata to {dc_file}...")
        with dc_file.open('w') as yaml_file:
            yaml.dump(dc_data, yaml_file, sort_keys=False)
        print_data = dc_data["services"]["generate-vectortiles"]["environment"].items()
        print(tabulate(print_data))

    return exit_code


def normalize_make_dc(area, min_zoom, max_zoom, ver):
    area = os.environ.get("OSM_AREA_NAME", "Unknown") if not area else area
    min_zoom = int(os.environ.get("MIN_ZOOM", 0) if min_zoom is None else min_zoom)
    max_zoom = int(os.environ.get("MAX_ZOOM", 7) if max_zoom is None else max_zoom)
    ver = float(os.environ.get("MAKE_DC_VERSION", 2) if ver is None else ver)
    return area, min_zoom, max_zoom, ver


async def main_async(args):
    if args["make-dc"]:
        return make_docker_compose_file(
            Path(args["<pbf-file>"]), Path(args["--make-dc"]),
            args['--minzoom'], args['--maxzoom'], args["--id"], args["--dc-ver"])

    urls, md5, repl_url = None, None, None
    exit_code = 0
    async with aiohttp.ClientSession(headers={'User-Agent': USER_AGENT}) as session:
        if args.list:
            if args.service != "geofabrik":
                raise SystemExit('List only supports geofabrik service for now')
            catalog = await get_geofabrik_list(session, args['--force-latest'],
                                               args['--no-cache'])
            info = [dict(id=v["full_id"], name=v["full_name"]) for v in
                    catalog.values()]
            print(tabulate(info, headers="keys") + '\n')
            return

        if args.planet:
            use_primary = args['--include-primary']
            urls, md5 = await Catalog().init(session, args.verbose, use_primary,
                                             args['--force-latest'])
            area_id = "planet"
            repl_url = "http://planet.openstreetmap.org/replication/day/"
        else:
            urls, md5, area_id, repl_url = await prepare_area_download(args, session)

    dry_run = args['--dry-run']
    if urls:
        exit_code = await run_aria2c(args.args, dry_run, md5, urls, args,
                                     area_id)
    if args['--imposm-cfg']:
        if not repl_url:
            raise SystemExit(f"Imposm config file cannot be generated in this mode")
        save_cfg_file(dry_run, args['--imposm-cfg'], repl_url, args['--kv'])

    return exit_code


async def run_aria2c(aria2c_args, dry_run, md5, urls, args, area_id):
    params = ['aria2c']
    if md5:
        params.append(f'--checksum=md5={md5}')
    if len(urls) > 1 and not any(
        (v for v in aria2c_args if v == '-s' or v.startswith('--split'))
    ):
        # user has not passed -s or --split, so use as many streams as urls
        params.append(f'--split={len(urls)}')
    if not any((v for v in aria2c_args if v.startswith('--http-accept-gzip'))):
        # user has not passed --http-accept-gzip, so always specify we accept gzip
        params.append('--http-accept-gzip')
    if not any((v for v in aria2c_args if v == '-U' or v.startswith('--user-agent'))):
        # user has not set a custom user agent, set one
        params.append(f'--user-agent={USER_AGENT}')

    extra_env = None
    if args['--make-dc']:
        for flag in ('--on-download-complete',):
            if any((v for v in aria2c_args if v.startswith(flag))):
                raise ValueError("Unable to use --make-dc together with "
                                 f"the {flag} aria2c parameter")
        area_id, min_zoom, max_zoom, dc_ver = normalize_make_dc(
            area_id, args['--minzoom'], args['--maxzoom'], args['--dc-ver'])
        extra_env = {
            "DOWNLOAD_OSM_DC_FILE": str(Path(args['--make-dc'])),
            "OSM_AREA_NAME": str(area_id),
            "MIN_ZOOM": str(min_zoom),
            "MAX_ZOOM": str(max_zoom),
            "MAKE_DC_VERSION": str(dc_ver),
        }
        params.append("--on-download-complete")
        params.append(__file__)

    params.extend(aria2c_args)
    params.extend(urls)
    print(f"\n  {subprocess.list2cmdline(params)}")
    if args.verbose and extra_env:
        env_str = ', '.join((f'{k}={v}' for k, v in extra_env.items()))
        print(f"  Setting environment vars: {env_str}")
    capture_output = False
    for flag in ('--on-bt-download-complete', '--on-download-pause',
                 '--on-download-complete', '--on-download-start',
                 '--on-download-error', '--on-download-stop'):
        if any((v for v in params if v.startswith(flag))):
            capture_output = True
            break
    if args.verbose:
        if capture_output:
            print("  capturing stdout/stderr to wait for subprocess exit")
        else:
            print("  aria2c output will be printed directly to terminal")
    # Make sure to print/flush everything to STDOUT before running subprocess
    print("", flush=True)

    if not dry_run:
        # Use capture_output to ensure that callback finishes before run() returns
        # This is only needed if any callbacks are used
        if extra_env:
            env = os.environ.copy()
            env.update(extra_env)
        else:
            env = None
        res = subprocess.run(params, env=env, capture_output=capture_output)
        ret = res.returncode
        if capture_output:
            stdout = res.stdout.decode('utf-8')
            if stdout:
                print(stdout)
            stderr = res.stderr.decode('utf-8')
            if stderr:
                print_err(stderr)
            # Callbacks do not report errors, so detect it
            if ret == 0 and stderr and "Traceback (most recent call last)" in stderr:
                ret = 1
        return ret
    else:
        print("Data is not downloaded because of the --dry-run parameter")
        if args['--make-dc']:
            print("docker-compose file generation was skipped")
        return 0


def main():
    if 'magic' not in docopt_funcs:
        print("""
Found invalid version of docopt. Must use docopt_ng instead. Uninstall it with
  $ python3 -m pip uninstall docopt
and re-install all required dependencies with
  $ python3 -m pip install -r requirements.txt
""")
        exit(1)

    dc_file = os.environ.get("DOWNLOAD_OSM_DC_FILE")
    if dc_file and len(sys.argv) == 4:
        # Skip a line after aria2c output
        print("\nRunning download-osm in aria2c callback mode...", flush=True)
        exit(make_docker_compose_file(Path(sys.argv[3]), Path(dc_file)))

    exit(asyncio.run(main_async(docopt(__doc__, version=openmaptiles.__version__))))


if __name__ == '__main__':
    main()
