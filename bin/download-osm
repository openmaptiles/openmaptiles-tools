#!/usr/bin/env python
"""
Usage:
  download-osm planet [--verbose] [--include-primary] [ -- any aria2c parameters ]
  download-osm --help
  download-osm --version

Options:
  --include-primary     If set, will download from the main osm.org (please avoid
                        using this parameter to reduce the load on the primary server)
  -v --verbose          Print additional debugging information
  --help                Show this screen.
  --version             Show version.

Any aria2c parameters can be set after the '--' string. For example, set download dir:
    download-osm planet -- -d ./download
By default, aria2c is executed with --checksum (md5 hash) and --split parameters.
Split is used to download with as many streams, as download-osm finds.
Use  --split  or  -s  parameter to override that number.
"""

import asyncio
import sys
from collections import defaultdict
from dataclasses import dataclass
from datetime import datetime

import aiohttp
import re

import subprocess
from aiohttp import ClientSession
from bs4 import BeautifulSoup
# noinspection PyProtectedMember
from docopt import docopt, DocoptExit
from typing import List, Dict, Tuple

from tabulate import tabulate

import openmaptiles
from openmaptiles.utils import print_err


class Catalog:
    def __init__(self):
        self.sites = [
            Site('GB', 'https://planet.openstreetmap.org/pbf/', avoid_by_default=True),
            Site('DE', 'https://download.bbbike.org/osm/planet/'),
            Site('DE', 'https://ftp.spline.de/pub/openstreetmap/pbf/'),
            Site('DE', 'https://ftp5.gwdg.de'
                       '/pub/misc/openstreetmap/planet.openstreetmap.org/pbf/'),
            Site('JP', 'https://planet.passportcontrol.net/pbf/'),
            Site('NL', 'https://ftp.nluug.nl/maps/planet.openstreetmap.org/pbf/'),
            Site('NL', 'https://ftp.snt.utwente.nl/pub/misc/openstreetmap/'),
            Site('TW', 'https://free.nchc.org.tw/osm.planet/pbf/'),
            Site('US', 'https://ftp.osuosl.org/pub/openstreetmap/pbf/'),
            Site('US', 'https://ftpmirror.your.org/pub/openstreetmap/pbf/'),
        ]

    async def init(self, verbose: bool, use_primary: bool) -> Tuple[List[str], str]:
        """Load the list of all available sources from all mirror sites,
        and pick the most recent file that is widely available.
        Returns a list of urls and the expected md5 hash.
        """
        sources_by_hash: Dict[str, List[Source]] = defaultdict(list)
        async with aiohttp.ClientSession(headers={
            'User-Agent': 'OpenMapTiles download-osm '
                          '(https://github.com/openmaptiles/openmaptiles-tools)'
        }) as session:
            await asyncio.wait([v.init(session, verbose) for v in self.sites])
        for site in self.sites:
            for s in site.sources:
                sources_by_hash[s.hash].append(s)
        # Remove "latest" from sources if they have no md5 hash
        # noinspection PyTypeChecker
        no_hash_sources = sources_by_hash.pop(None, [])

        ts_to_hash = self.get_attr_to_hash(sources_by_hash, 'timestamp', 'file date')
        if not ts_to_hash:
            raise ValueError(f"Unable to consistently load data - dates don't match")

        # Sources without md5 can only be used if there is one unique length per md5.
        # If more than one hash has the same length (e.g. download sizes didn't change),
        # we don't know which is which, so we have to ignore them.
        len_to_hash = self.get_attr_to_hash(sources_by_hash, 'file_len', 'length')
        if len_to_hash or not no_hash_sources:
            for src in no_hash_sources:
                if src.file_len in len_to_hash:
                    src.hash = len_to_hash[src.file_len]
                    sources_by_hash[src.hash].append(src)
                else:
                    print(f"Source {src} has unrecognized file length={src.file_len}")
        else:
            print(f"Unable to use sources - unable to match 'latest' without date/md5:")
            for s in no_hash_sources:
                print(s)

        # Pick the best hash to download - should have the largest timestamp,
        # but if the count is too low, use the second most common timestamp.
        stats = [(v[0].timestamp, len(v), v[0].hash)
                 for v in sources_by_hash.values()]
        stats.sort(reverse=True)

        print("Latest files available:")
        info = [dict(date=f"{s[0]:%Y-%m-%d}", number_of_sites=s[1], md5=s[2])
                for s in stats]
        print(tabulate(info, headers="keys") + '\n')

        if len(stats) > 1 and stats[0][1] * 1.5 < stats[1][1]:
            hash_to_download = stats[1][2]
            info = f" because the latest {stats[0][0]:%Y-%m-%d} is not widespread yet"
        else:
            hash_to_download = stats[0][2]
            info = ""

        src_list = sources_by_hash[hash_to_download]
        ts = next((v.timestamp for v in src_list if v.timestamp), None)
        ts = f"{ts:%Y-%m-%d}" if ts else "latest (unknown date)"

        if len(src_list) > 2 and not use_primary:
            src_list = [v for v in src_list if not v.site.avoid_by_default]

        print(f"Will download {ts} "
              f"(size={src_list[0].file_len:,} B, md5={src_list[0].hash}, "
              f"regions={','.join(sorted((v.site.country for v in src_list)))}){info}")
        if verbose:
            info = [dict(country=s.site.country, url=s.url) for s in src_list]
            print(tabulate(info, headers="keys") + '\n')

        return [s.url for s in src_list], hash_to_download

    @staticmethod
    def get_attr_to_hash(sources_by_hash, attr_name, attr_desc):
        """Verify that a specific attribute is unique per hash in all sources"""
        attr_to_hash = {}
        for sources in sources_by_hash.values():
            for source in sources:
                attr = getattr(source, attr_name)
                if attr is None:
                    continue
                if attr not in attr_to_hash:
                    attr_to_hash[attr] = source.hash
                elif attr_to_hash[attr] != source.hash:
                    print(f"Multiple files with the same {attr_desc} have different "
                          f"hashes:")
                    print(f"* {source}, {attr_desc}={attr}, hash={source.hash}")
                    src = sources_by_hash[attr_to_hash[attr]][0]
                    print(f"* {src}, {attr_desc}={(getattr(src, attr))}, "
                          f"hash={src.hash}")
                    return None
        return attr_to_hash


class Site:
    re_name = re.compile(r'^planet-(\d{6}|latest)\.osm\.pbf(\.md5)?$')

    def __init__(self, country, url, avoid_by_default=False):
        self.country: str = country
        self.url: str = url
        self.avoid_by_default: bool = avoid_by_default
        self.sources: List[Source] = []

    async def init(self, session: ClientSession, verbose: bool):
        """initialize the self.sources with the relevant Source objects
        by parsing the mirror's site HTML page, and getting all <a> tags"""
        try:
            soup = BeautifulSoup(await fetch(session, self.url), 'html.parser')
            sources = self.parse_hrefs(
                [(v.text.strip(), v['href'].strip())
                 for v in soup.find_all('a') if 'href' in v.attrs],
                verbose)
            await asyncio.wait([v.load_hash(session) for v in sources] +
                               [v.load_metadata(session) for v in sources])
            sources = [s for s in sources if s.is_valid]
            if not sources:
                raise ValueError(f"No sources found")
            if len(sources) > 1 and sources[0].hash == sources[1].hash:
                del sources[0]  # latest is the same as the last one
            self.sources = sources
        except Exception as ex:
            print_err(f"Unable to use {self.country} source {self.url}: {ex}")

    def parse_hrefs(self, items: List[tuple], verbose) -> List['Source']:
        """Convert a list of (name, href) tuples to a list of valid sources,
        including only the two most recent ones, plus the 'latest' if available."""
        all_sources: Dict[str, Source] = {}
        for name, href in sorted(items):
            m = self.re_name.match(name)
            if not m:
                if verbose:
                    print(f"Unexpected name '{name}' from {self.url}")
                continue
            try:
                url = href if '/' in href else (self.url + href)
                date = m.group(1)
                is_md5 = bool(m.group(2))
                dt = None if date == 'latest' else datetime.strptime(date, '%y%m%d')
                if not is_md5:
                    if date in all_sources:
                        raise ValueError(f"{date} already already exists")
                    all_sources[date] = Source(self, name, url, dt)
                else:
                    if date not in all_sources:
                        raise ValueError(f"md5 file exists, but data file does not")
                    all_sources[date].url_hash = url
            except Exception as ex:
                print_err(f'Error: {ex}, while parsing {name} from {self.url}')

        # get the last 2 sources that have dates in the name, as well as the "latest"
        latest = all_sources.pop('latest', None)
        result = [all_sources[k]
                  for k in list(sorted(all_sources.keys(), reverse=True))[:2]]
        if latest:
            result.insert(0, latest)
        return result


@dataclass
class Source:
    site: Site
    text: str
    url: str
    timestamp: datetime
    url_hash: str = None
    hash: str = None
    file_len: int = None
    is_valid: bool = False

    def __str__(self):
        return f"{self.text} from {self.url} ({self.site.country})"

    async def load_hash(self, session: ClientSession):
        if self.url_hash:
            self.hash = (await fetch(session, self.url_hash)).strip().split(' ')[0]

    async def load_metadata(self, session: ClientSession):
        if not self.url:
            return
        try:
            async with session.head(self.url) as resp:
                if resp.status >= 400:
                    raise ValueError(f"Status={resp.status} for HEAD req {self.url}")
                if 'Content-Length' in resp.headers:
                    self.file_len = int(resp.headers['Content-Length'])
                    self.is_valid = True
        except Exception as ex:
            print_err(f"Unable to load metadata for {self}: {ex}")


async def fetch(session: ClientSession, url: str):
    async with session.get(url) as resp:
        if resp.status >= 400:
            raise ValueError(f"Received status={resp.status} loading {url}")
        return await resp.text()


async def main(aria2c_args, args):
    # Workaround for docopt limitation:
    # We need to print  [ -- any aria2c parameters ]  in the help message,
    # but not allow any of those words as parameters.
    if args['any'] or args['aria2c'] or args['parameters']:
        raise DocoptExit()

    verbose = args['--verbose']
    use_primary = args['--include-primary']

    urls, md5 = await Catalog().init(verbose, use_primary)

    params = ['aria2c', f'--checksum=md5={md5}']
    if not any((v for v in aria2c_args if re.match(r'^[\'"]*(-s|--split)', v))):
        # user has not passed -s or --split, so set it the same url count
        params.append(f'--split={len(urls)}')
    params.extend(aria2c_args)
    params.extend(urls)

    print('---- RUNNING aria2c ----')
    if verbose:
        print(f"Command line:\n  {subprocess.list2cmdline(params)}\n")
    res = subprocess.run(params)
    exit(res.returncode)


if __name__ == '__main__':
    argv = sys.argv[1:]
    argv_aria2c = []
    if '--' in argv:
        argv_aria2c = argv[argv.index('--') + 1:]
        argv = argv[:argv.index('--')]
    asyncio.run(
        main(argv_aria2c,
             docopt(__doc__, argv=argv, version=openmaptiles.__version__)))
