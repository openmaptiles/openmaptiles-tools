#!/usr/bin/env python3
"""
Usage:
  download-osm list <service> [-l] [-c]
  download-osm planet [-o <file> [-f]] [-j <file> [-k <kv>]...] [-d <file> [-i <z>]
               [-l] [-p] [-n] [-v] [-a <z>] [-x <v>]] [--] [<aria2c_args>...]
  download-osm [url|geofabrik|osmfr|bbbike] <area_or_url> [-o <file> [-f]] [-s <file>]
               [-j <file> [-k <kv>]...] [-l] [-c] [-n] [-v]
               [-d <file> [-i <z>] [-a <z>] [-x <v>] [--id <id>]] [--] [<aria2c_args>...]
  download-osm make-dc <pbf-file> -d <file> [-i <z>] [-a <z>] [-x <v>] [--id <id>]
  download-osm --help
  download-osm --version

Download types:
  planet           Loads latest planet file (50+ GB) from all known mirrors
  list <service>   Show available areas for a service. For now only 'geofabrik'.
  url <url>        Loads file from the specific <url>.
                   If <url>.md5 is available, download-osm will use it to validate.
  geofabrik <id>   Loads file from Geofabrik by ID, where the ID is either
                   "australia-oceania/new-zealand" or just "new-zealand".
                   See https://download.geofabrik.de/
  bbbike <id>      Loads file from BBBike by extract ID, for example "Austin".
                   See https://download.bbbike.org/osm/
  osmfr <id>       Loads file from openstreetmap.fr by extract ID, for example
                   "central-america/costa_rica".  Download will add '-latest.osm.pbf'.
                   See https://download.openstreetmap.fr/extracts
  make-dc          Analyze area data pbf file and generate Docker-Compose file with
                   metadata, e.g. bounding box.  This action can be combined with
                   the area downloads by setting the -d <file> parameter.

Options:
  -o --output <file>    Configure aria2c to save downloaded file under a given name.
                        If directory does not exist, it will be created.
                        If the file already exists, exits with an error, unless --force.
  -f --force            If set and file already exists, will delete it before downloading.
  -p --include-primary  If set, will download from the main osm.org (please avoid
                        using this parameter to reduce the load on the primary server)
  -l --force-latest     Always download the very latest available planet file,
                        even if there are too few mirrors that have it.
                        For area services, forces catalog re-download.
  -c --no-cache         Do not cache downloaded list of extracts
  -s --state <file>     Download state file and save it to the <file>
  -j --imposm-cfg <f>   Create an imposm configuration json file, and write
                        replication_url param into it.  Use -k any additional values.
                        (requires planet, geofabrik, or osmfr download type)
  -k --kv <key=value>   Add extra key=value params to the imposm config (requires -j)
  -d --make-dc <file>   Create a docker-compose file with the area metadata.
  -x --dc-ver <ver>     Set docker-compose file version to this value.
                        By default will be 2 unless MAKE_DC_VERSION env var is set.
  -i --minzoom=<zoom>   Set minimum zoom when making docker compose file.
                        By default will be 0 unless MIN_ZOOM env var is set.
  -a --maxzoom=<zoom>   Set maximum zoom when making docker compose file.
                        By default will be 7 unless MAX_ZOOM env var is set.
  --id=<area_id>        Name of the area. If not set, will use filename.
                        By default will be pbf file unless OSM_AREA_NAME env var is set.
  -n --dry-run          If set, do all the steps except the actual data download.
                        State file will still be downloaded if --state is set.
  -v --verbose          Print additional debugging information
  --help                Show this screen.
  --version             Show version.

Any parameters after empty ' -- ' param will be passed to aria2c. For example,
this sets the number of maximum concurrent downloads:
    download-osm planet -- --max-concurrent-downloads=2
See https://aria2.github.io/manual/en/html/aria2c.html#options for more options.
By default, aria2c is executed with --checksum (md5 hash) and --split parameters.
Split is used to download with as many streams, as download-osm finds.
Use  --split  or  -s  parameter to override that number. Use --dry-run
to see all parameters downloader will use with aria2c without downloading.

Internal callback mode:  If DOWNLOAD_OSM_DC_FILE env var is set, expects 3 parameters,
where the 3rd parameter is the name of the PDF file. Will run make-dc command.
"""

import asyncio
import json
import os
import re
import subprocess
import sys
from asyncio import sleep, Future
from collections import defaultdict
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Tuple, Iterable, Union

import aiohttp
import yaml
from aiohttp import ClientSession
from bs4 import BeautifulSoup
# noinspection PyProtectedMember
from docopt import docopt, __all__ as docopt_funcs
from tabulate import tabulate

import openmaptiles
from openmaptiles.utils import print_err

USER_AGENT = f'OpenMapTiles download-osm {openmaptiles.__version__} ' \
             '(https://github.com/openmaptiles/openmaptiles-tools)'


class Catalog:
    def __init__(self):
        self.mirrors = [
            MirrorMult('GB', 'https://planet.openstreetmap.org/pbf/', is_primary=True),
            Mirror('DE',
                   'https://download.bbbike.org/osm/planet/planet-latest.osm.pbf'),
            MirrorMult('DE', 'https://ftp.spline.de/pub/openstreetmap/pbf/'),
            MirrorMult('DE', 'https://ftp5.gwdg.de'
                             '/pub/misc/openstreetmap/planet.openstreetmap.org/pbf/'),
            # https://planet.passportcontrol.net/pbf/ redirects to broken SSL cert
            # when used from some locations.
            # See https://twitter.com/IchikawaYukko/status/1261541590566223872
            MirrorMult('JP', 'https://planet.passportcontrol.net/pbf/'),
            MirrorMult('DE', 'https://ftp.fau.de/osm-planet/pbf/'),
            Mirror('NL', 'https://ftp.nluug.nl/maps/planet.openstreetmap.org'
                         '/pbf/planet-latest.osm.pbf'),
            Mirror('NL', 'https://ftp.snt.utwente.nl'
                         '/pub/misc/openstreetmap/planet-latest.osm.pbf'),
            MirrorMult('TW', 'https://free.nchc.org.tw/osm.planet/pbf/'),
            Mirror('US', 'https://ftp.osuosl.org'
                         '/pub/openstreetmap/pbf/planet-latest.osm.pbf'),
            MirrorMult('US', 'https://ftpmirror.your.org/pub/openstreetmap/pbf/'),

            # Internet archive is a bit tricky, so ignoring for now. PRs welcome.
            #   https://archive.org/details/osmdata
        ]

    async def init(self,
                   session: ClientSession, verbose: bool,
                   use_primary: bool, force_latest: bool) -> Tuple[List[str], str]:
        """Load the list of all available sources from all mirrors,
        and pick the most recent file that is widely available.
        Returns a list of urls and the expected md5 hash.
        """
        print("Retrieving available files...")
        await sleep(0.01)  # Make sure the above print statement prints first
        sources_by_hash: Dict[str, List[Source]] = defaultdict(list)
        await asyncio.wait([v.init(session, verbose) for v in self.mirrors])
        for mirror in self.mirrors:
            for s in mirror.sources:
                sources_by_hash[s.hash].append(s)
        # Remove "latest" from sources if they have no md5 hash
        # noinspection PyTypeChecker
        no_hash_sources = sources_by_hash.pop(None, [])

        ts_to_hash = self.get_attr_to_hash(sources_by_hash, 'timestamp', 'file date')
        if not ts_to_hash:
            raise ValueError(f"Unable to consistently load data - dates don't match")

        # Sources without md5 can only be used if there is one unique length per md5.
        # If more than one hash has the same length (e.g. download sizes didn't change),
        # we don't know which is which, so we have to ignore them.
        len_to_hash = self.get_attr_to_hash(sources_by_hash, 'file_len', 'length')
        if len_to_hash or not no_hash_sources:
            for src in no_hash_sources:
                if src.file_len in len_to_hash:
                    src.hash = len_to_hash[src.file_len]
                    sources_by_hash[src.hash].append(src)
                else:
                    print(f"WARN: Source {src} has unrecognized file length=" +
                          ("unknown" if src.file_len is None else f"{src.file_len:,}"))
        else:
            print(f"Unable to use sources - unable to match 'latest' without date/md5:")
            for s in no_hash_sources:
                print(s)

        # Pick the best hash to download - should have the largest timestamp,
        # but if the count is too low, use the second most common timestamp.
        for hsh in sources_by_hash:
            # Some sources just have "latest", so for each hash try to get a real date
            # by sorting real dates before the None timestamp,
            # and with the non-None file sizes first.
            sources_by_hash[hsh].sort(
                key=lambda v: (v.timestamp or datetime.max, v.file_len or (1 << 63)))
        # Treat "latest" (None) timestamp as largest value (otherwise sort breaks)
        stats = [(v[0].timestamp or datetime.max, len(v), v[0])
                 for v in sources_by_hash.values()]
        stats.sort(reverse=True)

        print("\nLatest available files:\n")
        info = [dict(date=f"{s[0]:%Y-%m-%d}" if s[0] < datetime.max else "Unknown",
                     mirror_count=s[1], md5=s[2].hash,
                     size=s[2].size_str())
                for s in stats]
        print(tabulate(info, headers="keys") + '\n')

        if not force_latest and len(stats) > 1 and stats[0][1] * 1.5 < stats[1][1]:
            hash_to_download = stats[1][2].hash
            info = f" because the latest {stats[0][0]:%Y-%m-%d} is not widespread yet"
        else:
            hash_to_download = stats[0][2].hash
            info = ""

        src_list = sources_by_hash[hash_to_download]
        ts = next((v.timestamp for v in src_list if v.timestamp), None)
        ts = f"{ts:%Y-%m-%d}" if ts else "latest (unknown date)"

        if len(src_list) > 2 and not use_primary:
            src_list = [v for v in src_list if not v.mirror.is_primary]
            info = ' (will not use primary)' + info

        print(f"Will download planet published on {ts}, "
              f"size={src_list[0].size_str()}, md5={src_list[0].hash}, "
              f"using {len(src_list)} sources{info}")
        if verbose:
            print(
                tabulate([dict(country=s.mirror.country, url=s.url) for s in src_list],
                         headers="keys") + '\n')

        return [s.url for s in src_list], hash_to_download

    @staticmethod
    def get_attr_to_hash(sources_by_hash, attr_name, attr_desc):
        """Verify that a specific attribute is unique per hash in all sources"""
        attr_to_hash = {}
        for sources in sources_by_hash.values():
            for source in sources:
                attr = getattr(source, attr_name)
                if attr is None:
                    continue
                if attr not in attr_to_hash:
                    attr_to_hash[attr] = source.hash
                elif attr_to_hash[attr] != source.hash:
                    print(f"ERROR: Multiple files with the same {attr_desc}"
                          f" have different hashes:")
                    print(f"* {source}, {attr_desc}={attr}, hash={source.hash}")
                    src = sources_by_hash[attr_to_hash[attr]][0]
                    print(f"* {src}, {attr_desc}={getattr(src, attr_name)}, "
                          f"hash={src.hash}")
                    return None
        return attr_to_hash


class Mirror:
    re_name = re.compile(r'^planet-(\d{6}|latest)\.osm\.pbf(\.md5)?$')

    def __init__(self, country, url, is_primary=False):
        self.country: str = country
        self.url: str = url
        self.is_primary: bool = is_primary
        self.sources: List[Source] = []

    async def init(self, session: ClientSession, verbose: bool):
        """initialize the self.sources with the relevant Source objects
        by parsing the mirror's HTML page, and getting all <a> tags"""
        try:
            sources = await self.get_sources(session, verbose)
            if not sources:
                raise ValueError(f"No sources found")
            await load_sources(sources, session, verbose)
            if len(sources) > 1 and sources[0].hash == sources[1].hash:
                del sources[0]  # latest is the same as the last one
            self.sources = sources
        except Exception as ex:
            print_err(f"Unable to use {self.country} source {self.url}: {ex}")

    async def get_sources(self, session, verbose):
        return [Source(name='planet-latest.osm.pbf', url=self.url, mirror=self)]


class MirrorMult(Mirror):
    """A mirror with multiple files"""

    async def get_sources(self, session, verbose):
        soup = BeautifulSoup(await fetch(session, self.url), 'html.parser')
        return self.parse_hrefs(
            [(v.text.strip(), v['href'].strip())
             for v in soup.find_all('a') if 'href' in v.attrs],
            verbose)

    def parse_hrefs(self, items: List[tuple], verbose) -> List['Source']:
        """Convert a list of (name, href) tuples to a list of valid sources,
        including only the two most recent ones, plus the 'latest' if available."""
        all_sources: Dict[str, Source] = {}
        for name, href in sorted(items):
            m = self.re_name.match(name)
            if not m:
                if verbose:
                    print(f"Ignoring unexpected name '{name}' from {self.url}")
                continue
            try:
                url = href if '/' in href else (self.url + href)
                date = m.group(1)
                is_md5 = bool(m.group(2))
                dt = None if date == 'latest' else datetime.strptime(date, '%y%m%d')
                if not is_md5:
                    if date in all_sources:
                        raise ValueError(f"{date} already already exists")
                    all_sources[date] = Source(name, url, dt, self)
                else:
                    if date not in all_sources:
                        raise ValueError(f"md5 file exists, but data file does not")
                    all_sources[date].url_hash = url
            except Exception as ex:
                print_err(f'WARN: {ex}, while parsing {name} from {self.url}')

        # get the last 2 sources that have dates in the name, as well as the "latest"
        latest = all_sources.pop('latest', None)
        result = [all_sources[k]
                  for k in list(sorted(all_sources.keys(), reverse=True))[:2]]
        if latest:
            result.insert(0, latest)
        return result


@dataclass
class Source:
    name: str
    url: str
    timestamp: datetime = None
    mirror: Mirror = None
    url_hash: str = None
    hash: str = None
    file_len: int = None

    def __post_init__(self):
        if self.url_hash is None:
            self.url_hash = self.url + ".md5"

    def __str__(self):
        return self.to_str()

    def to_str(self, use_hash_url=False):
        res = f"{self.name} from {self.url_hash if use_hash_url else self.url}"
        if self.mirror:
            res = f"{res} ({self.mirror.country})"
        return res

    async def load_hash(self, session: ClientSession, verbose: bool):
        if not self.url_hash:
            return
        try:
            if verbose:
                print(f"Getting md5 checksum from {self.url_hash}")
            hsh = (await fetch(session, self.url_hash)).strip().split(' ')[0]
            if not re.match(r'^[a-fA-F0-9]{32}$', hsh):
                raise ValueError(f"Invalid md5 hash '{hsh}'")
            self.hash = hsh
        except Exception as ex:
            print_err(f"Unable to load md5 hash for {self.to_str(True)}: {ex}")

    async def load_metadata(self, session: ClientSession, verbose: bool):
        if not self.url:
            return
        try:
            if verbose:
                print(f"Getting content length for {self.url}")
            async with session.head(self.url) as resp:
                if resp.status >= 400:
                    raise ValueError(f"Status={resp.status} for HEAD request")
                if 'Content-Length' in resp.headers:
                    self.file_len = int(resp.headers['Content-Length'])
        except Exception as ex:
            print_err(f"Unable to load metadata for {self}: {ex}")

    def size_str(self):
        if self.file_len is None:
            return "Unknown"
        return f"{self.file_len / 1024.0 / 1024:,.1f} MB ({self.file_len:,})"


def load_sources(sources: Iterable[Source], session: ClientSession, verbose: bool):
    return asyncio.wait([v.load_hash(session, verbose) for v in sources] +
                        [v.load_metadata(session, verbose) for v in sources])


async def fetch(session: ClientSession, url: str):
    async with session.get(url) as resp:
        if resp.status >= 400:
            raise ValueError(f"Received status={resp.status}")
        return await resp.text()


class AreaSource:

    def __init__(self, session: ClientSession, force_latest: bool, no_cache: bool):
        self.session = session
        self.force_latest = force_latest
        self.no_cache = no_cache
        self.name = type(self).__name__

    async def search(self, area_id: str, id2: str,
                     is_guessing: bool) -> Union[None, Tuple[Source, str, str]]:
        raise ValueError(f"Search is not implemented for {self.name}")

    async def get_catalog(self) -> dict:
        list_path = Path(__file__).parent / 'cache' / f'{self.name}.json'
        list_path.parent.mkdir(parents=True, exist_ok=True)
        data = None
        if not self.force_latest:
            try:
                data = list_path.read_text(encoding="utf-8")
            except FileNotFoundError:
                pass
        if data is None:
            print(f"Downloading the list of available {self.name} areas...")
            data = await self.fetch_catalog()
            if not self.no_cache:
                list_path.write_text(data, encoding='utf-8')
        catalog, warnings = await self.parse_catalog(data)
        if warnings:
            print(f"ERRORS parsing {self.name} catalog file:", file=sys.stderr)
            for warn in warnings:
                print(f" * {warn}", file=sys.stderr)
        return catalog

    def fetch_catalog(self) -> Future:
        raise ValueError(f"Catalog is not supported for {self.name}")

    async def parse_catalog(self, data) -> Tuple[Dict[str, str], List[str]]:
        raise ValueError(f"Catalog is not supported for {self.name}")


class Geofabrik(AreaSource):
    def fetch_catalog(self):
        return fetch(self.session, "https://download.geofabrik.de/index-v1-nogeom.json")

    async def get_printable_catalog(self):
        catalog = await self.get_catalog()
        return [dict(id=v["full_id"], name=v["full_name"]) for v in catalog.values()]

    async def parse_catalog(self, data):
        entries = {v["properties"]["id"]: v["properties"]
                   for v in json.loads(data)["features"]}
        warnings = []
        # resolve parents until no more changes are made
        for entry in entries.values():
            full_id = entry["id"]
            try:
                full_name = entry["name"]
                current = entry
                for idx in range(10):  # avoids infinite recursion
                    if "parent" in current:
                        current = entries[current["parent"]]
                        full_id = f"{current['id']}/{full_id}"
                        full_name = f"{current['name']} / {full_name}"
                    else:
                        break
                else:
                    raise ValueError(
                        f"Geofabrik data '{entry['id']}' has infinite parent loop")
                entry["full_id"] = full_id
                entry["full_name"] = full_name
                entry["url"] = entry["urls"]["pbf"]
                entry["url_updates"] = entry["urls"]["updates"]
            except KeyError as key:
                warnings.append(
                    f"Unable to parse entry {full_id} - key {key} does not exist")
        return {v["full_id"]: v
                for v in sorted(entries.values(), key=lambda v: v["full_id"])}, warnings

    async def search(self, area_id: str, id2: str, is_guessing: bool):
        catalog = await self.get_catalog()
        if area_id not in catalog:
            # If there is no exact match, find anything that ends with key
            reg = re.compile(".*" + re.escape("/" + area_id.lstrip("/")) + "$",
                             re.IGNORECASE)
            area_urls = [k for k in catalog.keys() if reg.match(k)]
            if not area_urls:
                error = f"ID '{area_id}' was not found in Geofabrik."
                if not is_guessing:
                    error += "\nUse 'list geofabrik' to see available extract, or try --force-latest to refresh the list of extracts."
            elif len(area_urls) > 1:
                variants = [(v, catalog[v]['full_name']) for v in area_urls]
                error = (f"More than one ID '{area_id}' was found in "
                         f"Geofabrik, use longer ID:\n{tabulate(variants)}")
            else:
                error = None
                area_id = area_urls[0]

            if error:
                if is_guessing:
                    print(error)
                    return None
                else:
                    raise SystemExit(error)
        url = catalog[area_id]["url"]
        return (Source(area_id, url),
                catalog[area_id]["url_updates"],
                url.replace("-latest.osm.pbf", "-updates/state.txt"))


class UrlSrc(AreaSource):
    async def search(self, val: str, id2: str, is_guessing: bool):
        if not is_guessing or val.startswith("https://") or val.startswith("http://"):
            return Source((id2 or "").strip() or "raw url", val), None, None
        return None


class Bbbike(AreaSource):
    url = "https://download.bbbike.org/osm/bbbike/"

    def fetch_catalog(self):
        return fetch(self.session, self.url)

    async def get_printable_catalog(self):
        catalog = await self.get_catalog()
        # BBBike has a nice info screen for each area
        return [dict(id=k, info="/".join(v.url.split("/")[:-1]) + "/")
                for k, v in catalog.items()]

    async def parse_catalog(self, data):
        soup = BeautifulSoup(data, 'html.parser')
        return {
                   v.text: Source(
                       v.text,
                       f"{self.url}{v['href']}{v['href'].rstrip('/')}.osm.pbf")
                   for v in soup.find_all('a')
                   if 'href' in v.attrs and v.text != ".."
               }, []

    async def search(self, area_id: str, id2: str, is_guessing: bool):
        catalog = await self.get_catalog()
        area_id = area_id.lower()
        for k, v in catalog.items():
            if area_id == k.lower():
                return v, None, None
        return None


class Osmfr(AreaSource):
    async def search(self, area_id: str, id2: str, is_guessing: bool):
        url = f"http://download.openstreetmap.fr/extracts/{area_id}-latest.osm.pbf"
        return (Source(area_id, url),
                f"http://download.openstreetmap.fr/replication/{area_id}/minute/",
                url.replace("-latest.osm.pbf", ".state.txt"))


area_sources = {
    # The keys must match the program parameters above.
    # The sources order is important
    "url": UrlSrc,
    "geofabrik": Geofabrik,
    "bbbike": Bbbike,
    "osmfr": Osmfr,
}


async def save_state_file(session, state_url, state_file):
    state_file = Path(state_file).resolve()
    print(f"Downloading state file {state_url} to {state_file}")
    data = await fetch(session, state_url)
    state_file.parent.mkdir(parents=True, exist_ok=True)
    state_file.write_text(data, encoding="utf-8")


def save_cfg_file(dry_run, cfg_file, repl_url, extras):
    cfg_file = Path(cfg_file).resolve()
    print(f"{'Would create' if dry_run else 'Creating'} Imposm config file {cfg_file}:")
    if repl_url.endswith("/minute/"):
        interval = "1m"
    elif repl_url.endswith("/hour/"):
        interval = "1h"
    else:
        interval = "24h"
    data = dict(replication_url=repl_url, replication_interval=interval)
    for kv in (extras or []):
        kv = kv.split('=', 1)
        if len(kv) != 2:
            raise SystemExit(
                f"--kv '{kv[0]}' must be in a 'key=value' format")
        data[kv[0]] = kv[1]
    print(tabulate(data.items()))
    if not dry_run:
        cfg_file.write_text(json.dumps(data, indent=2, sort_keys=True) + "\n",
                            encoding="utf-8")


class AreaParamParser:

    def __init__(self, session, force_latest, no_cache, args) -> None:
        self.args = args
        self.session = session
        self.area_id = self.args['<area_or_url>']
        self.repl_url = None
        self.state_url = None
        self.force_latest = force_latest
        self.no_cache = no_cache
        # Limit sources to just the one passed in the arguments, or keep all
        self.sources = {k: v for k, v in area_sources.items() if
                        self.args[k]} or area_sources
        self.is_guessing = len(self.sources) > 1

    async def parse(self):
        if self.is_guessing:
            print("Area source has not been specified. Auto-detecting...")
        id2 = self.args["--id"] or os.environ.get("OSM_AREA_NAME")
        for src_name, src_type in self.sources.items():
            site = src_type(self.session, self.force_latest, self.no_cache)
            res = await site.search(self.area_id, id2, self.is_guessing)
            if res:
                src, repl_url, state_url = res
                await self.load_source(src)
                if src.file_len is not None:
                    break
        else:
            raise SystemExit(f"Unable to auto-detect the source for '{self.area_id}'")
        if self.is_guessing:
            print(f"'{src.name}' was found in {src_name}.")
        print(f"Downloading {src.url} (size={src.size_str()}, md5={src.hash})",
              flush=True)
        if self.args.state:
            if not state_url:
                raise SystemExit(f"State is not available when using '{src_type}'")
            await save_state_file(self.session, state_url, self.args.state)
        return [src.url], src.hash, src.name, repl_url

    def load_source(self, source: Source):
        return load_sources([source], self.session, self.args.verbose)


def make_docker_compose_file(pbf_file: Path, dc_file: Path,
                             min_zoom=None, max_zoom=None, area=None, ver=None):
    area, min_zoom, max_zoom, dc_ver = normalize_make_dc(area, min_zoom, max_zoom, ver)
    print(f"Extracting metadata from {pbf_file} using osmconvert", flush=True)
    params = ["osmconvert", "--out-statistics", str(pbf_file)]
    res = subprocess.run(params, capture_output=True)
    exit_code = res.returncode
    if exit_code == 0:
        res_text = res.stdout.decode('utf-8')
        # Convert output to a dictionary with non-empty values
        res = {v[0]: v[1] for v in
               [[vv.strip() for vv in v.split(':', 1)] for v in res_text.split('\n')]
               if len(v) == 2 and all(v)}
        lon_min = res["lon min"]
        lon_max = res["lon max"]
        lat_min = res["lat min"]
        lat_max = res["lat max"]
        timestamp_max = res["timestamp max"]
        env_values = {
            "environment": {
                "BBOX": f"{lon_min},{lat_min},{lon_max},{lat_max}",
                "OSM_MAX_TIMESTAMP": timestamp_max,
                "OSM_AREA_NAME": area or pbf_file.name.split('.', 1)[0],
                "MIN_ZOOM": min_zoom,
                "MAX_ZOOM": max_zoom,
            }}
        dc_data = {
            "version": str(dc_ver),
            "services": {
                "openmaptiles-tools": env_values,
                "generate-vectortiles": env_values,
            }}
        print(f"Saving metadata to {dc_file}...")
        with dc_file.open('w') as yaml_file:
            yaml.dump(dc_data, yaml_file, sort_keys=False)
        print_data = dc_data["services"]["generate-vectortiles"]["environment"].items()
        print(tabulate(print_data))

    return exit_code


def normalize_make_dc(area, min_zoom, max_zoom, ver):
    area = os.environ.get("OSM_AREA_NAME", "Unknown") if not area else area
    min_zoom = int(os.environ.get("MIN_ZOOM", 0) if min_zoom is None else min_zoom)
    max_zoom = int(os.environ.get("MAX_ZOOM", 7) if max_zoom is None else max_zoom)
    ver = float(os.environ.get("MAKE_DC_VERSION", 2) if ver is None else ver)
    return area, min_zoom, max_zoom, ver


async def main_async(args):
    if args["make-dc"]:
        return make_docker_compose_file(
            Path(args["<pbf-file>"]), Path(args["--make-dc"]),
            args['--minzoom'], args['--maxzoom'], args["--id"], args["--dc-ver"])

    urls, md5, repl_url = None, None, None
    exit_code = 0
    async with aiohttp.ClientSession(headers={'User-Agent': USER_AGENT}) as session:
        force_latest = args['--force-latest']
        no_cache = args['--no-cache']
        if args.list:
            if args.service not in area_sources or args.service == "url":
                raise SystemExit(f'Unknown service name {args.service}')
            site = area_sources[args.service](session, force_latest, no_cache)
            try:
                info = await site.get_printable_catalog()
            except AttributeError:
                raise SystemExit(
                    f"Service {args.service} does not support listings.\n"
                    f"Please ask service maintainers to publish a catalog similar to Geofabrik's.")
            print(tabulate(info, headers="keys") + '\n')
            return

        if args.planet:
            use_primary = args['--include-primary']
            urls, md5 = await Catalog().init(session, args.verbose, use_primary,
                                             force_latest)
            area_id = "planet"
            repl_url = "http://planet.openstreetmap.org/replication/day/"
        else:
            urls, md5, area_id, repl_url = await AreaParamParser(
                session, force_latest, no_cache, args).parse()

    dry_run = args['--dry-run']
    if urls:
        exit_code = await run_aria2c(args['<aria2c_args>'], dry_run, md5, urls, args,
                                     area_id)
    if args['--imposm-cfg']:
        if not repl_url:
            raise SystemExit(f"Imposm config file cannot be generated from this source")
        save_cfg_file(dry_run, args['--imposm-cfg'], repl_url, args['--kv'])

    return exit_code


async def run_aria2c(aria2c_args, dry_run, md5, urls, args, area_id):
    params = ['aria2c']
    if md5:
        params.append(f'--checksum=md5={md5}')
    if len(urls) > 1 and not any(
        (v for v in aria2c_args if v == '-s' or v.startswith('--split'))
    ):
        # user has not passed -s or --split, so use as many streams as urls
        params.append(f'--split={len(urls)}')
    if not any((v for v in aria2c_args if v.startswith('--http-accept-gzip'))):
        # user has not passed --http-accept-gzip, so always specify we accept gzip
        params.append('--http-accept-gzip')
    if not any((v for v in aria2c_args if v == '-U' or v.startswith('--user-agent'))):
        # user has not set a custom user agent, set one
        params.append(f'--user-agent={USER_AGENT}')
    if args.output:
        assert_conflict_args(
            "--output", aria2c_args, '-d', '--dir', '-o', '--out', '-i', '--input-file',
            '--auto-file-renaming', '-Z', '--force-sequential', '--allow-overwrite')
        out_path = Path(args.output).resolve()
        out_path.parent.mkdir(parents=True, exist_ok=True)
        params.append(f'--dir={out_path.parent}')
        params.append(f'--out={out_path.name}')
        params.append('--auto-file-renaming=false')
        if args.force:
            params.append('--allow-overwrite=true')

    extra_env = None
    if args['--make-dc']:
        assert_conflict_args("--make-dc", aria2c_args, '--on-download-complete')
        area_id, min_zoom, max_zoom, dc_ver = normalize_make_dc(
            area_id, args['--minzoom'], args['--maxzoom'], args['--dc-ver'])
        extra_env = {
            "DOWNLOAD_OSM_DC_FILE": str(Path(args['--make-dc'])),
            "OSM_AREA_NAME": str(area_id),
            "MIN_ZOOM": str(min_zoom),
            "MAX_ZOOM": str(max_zoom),
            "MAKE_DC_VERSION": str(dc_ver),
        }
        params.append("--on-download-complete")
        params.append(__file__)

    params.extend(aria2c_args)
    params.extend(urls)
    print(f"\n  {subprocess.list2cmdline(params)}")
    if args.verbose and extra_env:
        env_str = ', '.join((f'{k}={v}' for k, v in extra_env.items()))
        print(f"  Setting environment vars: {env_str}")
    capture_output = False
    for flag in ('--on-bt-download-complete', '--on-download-pause',
                 '--on-download-complete', '--on-download-start',
                 '--on-download-error', '--on-download-stop'):
        if any((v for v in params if v.startswith(flag))):
            capture_output = True
            break
    if args.verbose:
        if capture_output:
            print("  capturing stdout/stderr to wait for subprocess exit")
        else:
            print("  aria2c output will be printed directly to terminal")
    # Make sure to print/flush everything to STDOUT before running subprocess
    print("", flush=True)

    if not dry_run:
        # Use capture_output to ensure that callback finishes before run() returns
        # This is only needed if any callbacks are used
        if extra_env:
            env = os.environ.copy()
            env.update(extra_env)
        else:
            env = None
        res = subprocess.run(params, env=env, capture_output=capture_output)
        ret = res.returncode
        if capture_output:
            stdout = res.stdout.decode('utf-8')
            if stdout:
                print(stdout)
            stderr = res.stderr.decode('utf-8')
            if stderr:
                print_err(stderr)
            # Callbacks do not report errors, so detect it
            if ret == 0 and stderr and "Traceback (most recent call last)" in stderr:
                ret = 1
        return ret
    else:
        print("Data is not downloaded because of the --dry-run parameter")
        if args['--make-dc']:
            print("docker-compose file generation was skipped")
        return 0


def assert_conflict_args(info, aria2c_args, *conflicted_args):
    for flag in conflicted_args:
        if any((v for v in aria2c_args if v.startswith(flag))):
            raise ValueError(f"Unable to use {info} together with "
                             f"the {flag} aria2c parameter")


def main():
    if 'magic' not in docopt_funcs:
        print("""
Found invalid version of docopt. Must use docopt_ng instead. Uninstall it with
  $ python3 -m pip uninstall docopt
and re-install all required dependencies with
  $ python3 -m pip install -r requirements.txt
""")
        exit(1)

    dc_file = os.environ.get("DOWNLOAD_OSM_DC_FILE")
    if dc_file and len(sys.argv) == 4:
        # Skip a line after aria2c output
        print("\nRunning download-osm in aria2c callback mode...", flush=True)
        exit(make_docker_compose_file(Path(sys.argv[3]), Path(dc_file)))

    exit(asyncio.run(main_async(docopt(__doc__, version=openmaptiles.__version__))))


if __name__ == '__main__':
    main()
