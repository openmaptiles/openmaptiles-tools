#!/usr/bin/env python
"""
Import names from the Wikidata Query Service

Usage:
  import-wikidata <tileset> [--storage=<table>]
                  [--pghost=<host>] [--pgport=<port>] [--dbname=<db>]
                  [--user=<user>] [--password=<password>]
                  [--cache <file> [--no-wdqs]] [--verbose]
  import-wikidata --table=<table>... [--storage=<table>]
                  [--pghost=<host>] [--pgport=<port>] [--dbname=<db>]
                  [--user=<user>] [--password=<password>]
                  [--cache <file> [--no-wdqs]] [--verbose]
  import-wikidata --help
  import-wikidata --version

  <tileset>             Tileset definition yaml file

Options:
  -t --table=<table>    Process specific table(s) rather than auto-detecting it from tileset.
  -w --storage=<table>  Write results to this table.  [default: wd_names]
  -c --cache <file>     Use a json cache file to store Wikidata labels.
  -n --no-wdqs          Do not use Wikidata Query Service to get new labels, only cache.
  -v --verbose          Print additional debugging information
  --help                Show this screen.
  --version             Show version.

PostgreSQL Options:
  -h --pghost=<host>    Postgres hostname. By default uses POSTGRES_HOST env or "localhost" if not set.
  -P --pgport=<port>    Postgres port. By default uses POSTGRES_PORT env or "5432" if not set.
  -d --dbname=<db>      Postgres db name. By default uses POSTGRES_DB env or "openmaptiles" if not set.
  -U --user=<user>      Postgres user. By default uses POSTGRES_USER env or "openmaptiles" if not set.
  --password=<password> Postgres password. By default uses POSTGRES_PASSWORD env or "openmaptiles" if not set.
"""
import json
from collections import defaultdict

import asyncio
import asyncpg
import requests
from asyncpg import Connection
from docopt import docopt, DocoptExit
from pathlib import Path
from typing import Iterable, List, Dict

import openmaptiles
from openmaptiles.pgutils import parse_pg_args, PgWarnings
from openmaptiles.tileset import Tileset
from openmaptiles.utils import batches


async def main(args):
    verbose = args['--verbose']
    storage_table = args['--storage']
    tileset_path = args['<tileset>']
    if tileset_path:
        tables = find_tables(tileset_path)
        if not tables:
            print(f"Unable to find any tables with wikidata fields in {tileset_path}")
            return
    elif not args['--table']:
        raise DocoptExit('--table parameter must specify the table to process')
    else:
        tables = args['--table']

    pghost, pgport, dbname, user, password = parse_pg_args(args)
    conn = await asyncpg.connect(
        database=dbname, host=pghost, port=pgport, user=user, password=password,
    )
    PgWarnings(conn)
    await conn.set_builtin_type_codec('hstore', codec_name='pg_contrib.hstore')

    cache_file = Path(args['--cache']) if args['--cache'] else None
    if cache_file:
        try:
            with cache_file.open('r', encoding='utf-8') as fp:
                cache = json.load(fp)
            print(f"Loaded {len(cache):,} items from cache {cache_file}")
        except FileNotFoundError:
            cache = {}
            print(f"Cache file {cache_file} does not exist, will create")
    else:
        cache = None

    print(f'Searching for Wikidata IDs in {len(tables)} tables...')
    ids = await find_needed_wd_ids(conn, tables, verbose)
    if cache:
        # Do not query for the IDs we already have in cache
        new_ids = set(ids).difference(cache.keys())
        print(f'Found {len(ids):,} Wikidata IDs, {len(ids) - len(new_ids):,} in cache, '
              f'{len(new_ids):,} new')
    else:
        new_ids = ids
        print(f'Found {len(ids):,} Wikidata IDs to load.')

    print(f'Deleting any existing data in {storage_table}...')
    await conn.execute(f'TRUNCATE TABLE {storage_table}')

    if cache:
        cached_ids = {k: {'name:' + kk: vv for kk, vv in cache[k].items()}
                      for k in cache.keys() if k in ids}
        if cached_ids:
            print(f'Copying {len(cached_ids):,} Wikidata IDs from cache...')
            await conn.copy_records_to_table(
                storage_table, columns=['id', 'labels'], records=cached_ids.items())
            print(f'Finished inserting {len(cached_ids):,} Wikidata IDs from cache '
                  f'into {storage_table} table')

    use_wdqs = not args['--no-wdqs']
    if new_ids and use_wdqs:
        print(f'Query Wikidata Query Service for {len(new_ids):,} IDs...')
        dups = await create_ids_from_wdqs(conn, new_ids, cache, storage_table, verbose)
        if dups:
            redirs = resolve_redirects(dups, verbose)
            if redirs:
                print(f'Querying Wikidata for {len(redirs):,} redirect IDs...')
                await create_ids_from_wdqs(conn, redirs, cache, storage_table, verbose)
    elif new_ids:
        print(f'There were {len(new_ids):,} new Wikidata IDs found, but they were not '
              f'retrieved from WDQS because of the --no-wdqs parameter')

    if cache is not None and use_wdqs and new_ids:
        # If using cache and there might have been changes, save them
        print(f"Saving {len(cache):,} items to cache {cache_file}")
        with cache_file.open('w', encoding='utf-8') as fp:
            json.dump(cache, fp, ensure_ascii=False, sort_keys=True, indent=1)


async def create_ids_from_wdqs(conn, ids, cache, storage_table, verbose):
    is_redirects = isinstance(ids, dict)
    missing = set(ids)

    for batch in batches(ids, 5000, lambda v: f'wd:{v}'):
        records = defaultdict(dict)

        def add_item(_qid, _lang, _label):
            records[_qid]['name:' + _lang] = _label
            if cache is not None:
                try:
                    labels = cache[_qid]
                except KeyError:
                    labels = {}
                    cache[_qid] = labels
                labels[_lang] = _label

        query = f'''\
    SELECT ?id ?label WHERE {{
      VALUES ?id {{ {' '.join(batch)} }}
      ?id rdfs:label ?label.
    }}'''
        if verbose:
            print(f"----------\n{query}\n----------")
        for row in wd_query(query):
            lang = row['label']['xml:lang']
            qid = entity_id(row['id'])
            missing.discard(qid)
            label = row['label']['value']
            if not is_redirects:
                add_item(qid, lang, label)
            else:
                # For redirects, cache will contain original QID -> labels
                # There could be more than one redirect to the same item,
                # in which case cache will have duplicates.
                for qid2 in ids[qid]:
                    add_item(qid2, lang, label)

        await conn.copy_records_to_table(
            storage_table, columns=['id', 'labels'], records=records.items())
    print(f'Inserted {len(ids) - len(missing):,} Wikidata IDs from WDQS '
          f'into {storage_table} table')
    return missing


def resolve_redirects(ids, verbose) -> Dict[str, List[str]]:
    missing = set(ids)
    redirects = defaultdict(list)
    print(f'Resolving {len(ids):,} possible Wikidata redirects...')
    for batch in batches(ids, 1000, lambda v: f'wd:{v}'):
        query = f'''\
    SELECT ?id ?id2 WHERE {{
      VALUES ?id {{ {' '.join(batch)} }}
      ?id owl:sameAs+ ?id2.
    }}'''
        if verbose:
            print(f"----------\n{query}\n----------")
        for row in wd_query(query):
            qid = entity_id(row['id'])
            redirects[entity_id(row['id2'])].append(qid)
            missing.discard(qid)
    print(f'{len(redirects):,} out of {len(ids):,} Wikidata ID redirects were found')
    if missing:
        print(f"{len(missing):,} Wikidata IDs were not found: {', '.join(missing)}")
    return redirects


def find_tables(tileset_path, table_prefix='osm_'):
    """Find all tables in the imposm mapping files that may contain Wikidata IDs"""
    tileset = Tileset.parse(tileset_path)
    tables = set()
    for layer in tileset.layers:
        for mapping in layer.imposm_mappings:
            for table_name, table_def in mapping['tables'].items():
                if (
                    'fields' not in table_def and
                    'columns' not in table_def
                ) or (
                    '_resolve_wikidata' in table_def and
                    not table_def['_resolve_wikidata']
                ):
                    continue
                # legacy support - imposm3 used to use "fields" instead of "columns"
                if 'fields' in table_def:
                    fields = table_def['fields']
                else:
                    fields = table_def['columns']
                if any((v for v in fields
                        if v['name'] == 'tags' and v['type'] == 'hstore_tags')):
                    tables.add(table_prefix + table_name)
    tables = list(sorted(tables))
    print(f"Found {len(tables)} tables with the 'tags' hstore fields")
    for table in tables:
        print(f"  * {table}")
    return tables


async def find_needed_wd_ids(conn: Connection, tables: Iterable[str],
                             verbose: bool) -> List[str]:
    ids = []
    part = [f"SELECT tags->'wikidata' AS id FROM {t} WHERE tags ? 'wikidata'"
            for t in tables]
    query = f"""\
SELECT DISTINCT id
FROM (({') UNION ('.join(part)})) as t
WHERE id SIMILAR TO 'Q[1-9][0-9]{{0,18}}'"""
    if verbose:
        print(f"----------\n{query}\n----------")
    for row in await conn.fetch(query):
        # Validate that the ID is an integer, and convert it back to Q-number strings
        ids.append(f"Q{int(row['id'][1:])}")
    ids.sort()
    return ids


def entity_id(column):
    return column['value'][len('http://www.wikidata.org/entity/'):]


def wd_query(sparql):
    r = requests.post(
        'https://query.wikidata.org/bigdata/namespace/wdq/sparql',
        data={'query': sparql},
        headers={
            'Accept': 'application/sparql-results+json',
            'User-Agent': f'OpenMapTiles OSM name resolver {openmaptiles.__version__}'
                          '(https://github.com/openmaptiles/openmaptiles)'
        })
    try:
        if not r.ok:
            print(r.reason)
            print(sparql)
            raise Exception(r.reason)
        return r.json()['results']['bindings']
    finally:
        r.close()


if __name__ == '__main__':
    asyncio.run(main(docopt(__doc__, version=openmaptiles.__version__)))
